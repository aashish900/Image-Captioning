# -*- coding: utf-8 -*-
"""Image Captioning New.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZFOq6A4IyPauer_IeRG3ILK2ShgDv4i2

### **Check GPU and RAM allocation**
"""

# memory footprint support libraries/code
!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi
!pip install gputil
!pip install psutil

!pip install humanize
import psutil
import humanize
import os
import GPUtil as GPU
GPUs = GPU.getGPUs()
# XXX: only one GPU on Colab and isnâ€™t guaranteed
gpu = GPUs[0]
def printm():
  process = psutil.Process(os.getpid())
  print("Gen RAM Free: " + humanize.naturalsize( psutil.virtual_memory().available ), " | Proc size: " + humanize.naturalsize( process.memory_info().rss))
  print("GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))
printm()

"""### **Download the Flicker8k_Dataset**"""

'''
Link to file in google drive
'''

# https://drive.google.com/open?id=1Gi4-xQ8zeJQNPNs2rEFc_0odasgcIs1c

'''
File ID = 1Gi4-xQ8zeJQNPNs2rEFc_0odasgcIs1c
'''
# Download data from drive
!wget --load-cookies /tmp/cookies.txt "https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1Gi4-xQ8zeJQNPNs2rEFc_0odasgcIs1c' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\1\n/p')&id=1Gi4-xQ8zeJQNPNs2rEFc_0odasgcIs1c" -O Image && rm -rf /tmp/cookies.txt

!ls

!mv Image Image_Captioning.zip

!ls

"""### **Unzip the Dataset**"""

!unzip Image_Captioning

!ls

!ls Image

!ls Image/Flickr8k_text

"""### **Import Relevant Libraries**"""

import numpy as np
import pandas as pd
import matplotlib
import matplotlib.pyplot as plt
import os
import string
import datetime as dt
import tensorflow as tf
from copy import copy
from keras.backend.tensorflow_backend import set_session
import keras
import pickle
import sys, time, os, warnings 
from collections import Counter
# %matplotlib inline

"""### **Save the path to the Dataset Directory**"""

## The location of the Flickr8k_ photos
dir_Flickr_jpg = "Image/Flicker8k_Dataset/"
## The location of the caption file
dir_Flickr_texts = "Image/Flickr8k_text/"
dir_Flickr_text = "Image/Flickr8k_text/Flickr8k.token.txt"

jpgs = os.listdir(dir_Flickr_jpg)
txts = os.listdir(dir_Flickr_texts)
print("The number of jpg file in Flicker8k: {}".format(len(jpgs)))
print("The number of txt file in Flicker8k: {}".format(len(txts)))

"""##**Preparing the Text**

###**Load the file**
"""

# Load doc into memory
def load_doc(filename):
   # Open the file as read only
    file = open(filename, 'r')
    # Read all text
    text = file.read()
    # Close the file
    file.close()
    return text
  
filename = dir_Flickr_text
# Load descriptions
doc = load_doc(filename)

doc[:100]

"""###**Load Description**"""

# Extract deacription for Images
def load_descriptions(doc):
  mapping = dict()
  # Process lines
  for line in doc.split('\n'):
    # Split line by white space
    tokens = line.split()
    # If the line doesn't have either the ID or description than discard it
    if len(line) < 2:
      continue
    # Take the first token as the image id, the rest as the description
    image_id, image_desc = tokens[0], tokens[1:]
    # Remove fileanme form imaeg id
    image_id = image_id.split('.')[0]
    # Convert description tokens back to string
    image_desc = ' '.join(image_desc)
    # Store the first description for each image
    if image_id not in mapping:
      mapping[image_id] = image_desc
  return mapping

# Parse descriptions
descriptions = load_descriptions(doc)
print('Loaded: %d' % len(descriptions))

"""###**Clean Descriptions**




*   **Convert all words to lower case**
*   **Remove all punctuations**
*   **Remove all characters that are one character or less in length (e.g. 'a')**
"""

import string

def clean_descriptions(descriptions):
  # Prepare translation table for removing punctuation
  table = str.maketrans('', '', string.punctuation)
  for key, desc in descriptions.items():
    # Tokenize
    desc = desc.split()
    # Convert to lower case
    desc = [word.lower() for word in desc]
    # Remove punctuation from each token
    desc = [w.translate(table) for w in desc]
    # Remove hanging 's and 'a
    desc = [word for word in desc if len(word) > 1]
    # Store as string
    descriptions[key] = ' '.join(desc)

# Clean descriptions
clean_descriptions(descriptions)
# Summarize vocabulary
all_tokens = ' '.join(descriptions.values()).split()
vocabulary = set(all_tokens)
print('Vocabulary Size: %d' % len(vocabulary))

"""###**Save Descriptions**"""

# Save descriptions to file, one per line
def save_doc(descriptions, filename):
  lines = list()
  for key, desc in descriptions.items():
    lines.append(key + ' ' + desc)
  data = '\n'.join(lines)
  file = open(filename, 'w')
  file.write(data)
  file.close()

# Save descriptions
save_doc(descriptions, 'descriptions.txt')

print(descriptions.items())

"""##**Preparing the Photos**"""

from os import listdir
from pickle import dump
from keras.applications.vgg16 import VGG16
from keras.preprocessing.image import load_img
from keras.preprocessing.image import img_to_array
from keras.applications.vgg16 import preprocess_input
from keras.layers import Input

# Extract features from each photo in the directory
def extract_features(directory):
  # Load the model
  in_layer =Input(shape = (224, 224, 3))
  model = VGG16(include_top = False, input_tensor = in_layer)
  print(model.summary())
  # Extract features from each photo
  features = dict()
  for name in listdir(directory):
    # Load an image form file
    filename = directory + '/' + name
    image = load_img(filename, target_size = (224, 224))
    # Convert the image pixels to numpy array
    image = img_to_array(image)
    # Reshape data for the model
    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))
    # Prepare the image for the VGG model
    image = preprocess_input(image)
    # Get features
    feature = model.predict(image, verbose = 0)
    # get image id
    image_id = name.split('.')[0]
    # Store festure
    features[image_id] = feature
    print('>%s' % name)
  return features

# Extract features from all images
directory = dir_Flickr_jpg
features = extract_features(directory)
print('Extracted Features: %d' % len(features))
# Save to file
# Save the features in byte format to features.pkl file
# dump takes object and the file you want to pickle to as its arguments
# wb ---> w stands for write, b stands for byte (storing in byte format)
dump(features, open('features.pkl', 'wb'))

"""##**Model**

###**Load Data**
"""

# LOad a pre-defined list of photo identifiers
def load_set(filename):
  doc = load_doc(filename)
  dataset = list()
  # Process line by line
  for line in doc.split('\n'):
    # Skip empty lines
    if len(line) < 1:
      continue
    # Get the image identifier
    identifier = line.split('.')[0]
    dataset.append(identifier)
  return set(dataset)

"""###**Split Dataset into Train and Test**"""

# split a dataset into train/test elements
def train_test_split(dataset):
  # Order keyas so the split is consistent
  ordered = sorted(dataset)
  # Return split dataset as two new sets
  return set(ordered[:100]), set(ordered[100: 200])

"""###**Load clean descriptions into memory**"""

# Load clean descriptions into memory
def load_clean_descriptions(filename, dataset):
  # Load document
  doc = load_doc(filename)
  descriptions = dict()
  for line in doc.split('\n'):
    # Split line by white space
    tokens = line.split()
    # Split id from description
    image_id, image_desc = tokens[0], tokens[1:]
    # Skip images that are not in the set
    if image_id in dataset:
      # Store
      descriptions[image_id] = 'starseq' + ' '.join(image_desc) + 'endseq'
    return descriptions

"""###**Load photo features**"""

# Load photo features
def load_photo_features(filename, dataset):
  # Load all features
  all_features = pickle.load(open(filename, 'rb'))
  # Filter features
  features = {k: all_features[k] for k in dataset}
  return features

"""###**Run the above functions**"""

filename = "Image/Flickr8k_text/Flickr_8k.devImages.txt"
dataset = load_set(filename)
print('Dataset: %d' % len(dataset))
# Train-Test split
train, test = train_test_split(dataset)
print('Train=%d, Test=%d' % (len(train), len(test)))
# Descriptions
train_descriptions = load_clean_descriptions('descriptions.txt', train)
test_descriptions = load_clean_descriptions('descriptions.txt', test)
print('Descriptions: train=%d, test=%d' % (len(train_descriptions), len(test_descriptions)))
# Photo features
train_features = load_photo_features('features.pkl', train)
test_features = load_photo_features('features.pkl', test)
print('Photos: train=%d, test=%d' % (len(train_features), len(test_features)))

"""###**Encode Words**"""

# Fit a tokenizer given caption descriptions
def create_tokenizer(descriptions):
  lines = list(descriptions.values())
  tokenizer = keras.preprocessing.text.Tokenizer()
  tokenizer.fit_on_texts(lines)
  return tokenizer

# Prepare tokenizer
tokenizer = create_tokenizer(descriptions)
vocab_size = len(tokenizer.word_index) + 1
print('Vocabulary Size: %d' % vocab_size)

"""###**Create Sequence**"""

# Create sequence of images, input sequences and output words for an image
def create_sequences(tokenizer, dec, image, max_length):
  Ximages, XSeq, y = list(), list(), list()
  vocab_size = len(tokenizer, word, index) + 1
  # Integer encode the description
  seq = tokenizer.texts_to_sequence([desc])[0]
  # Split one sequence into multiple X,y pairs
  for i in range*1, len(seq):
    # Select
    in_seq, out_seq = seq[:i], seq[i]
    # Pad input sequence
    in_seq = pad_sequences([in_seq], maxlen = max_length)[0]
    # Encode output sequence
    out_seq = to_categorical([out_seq], num_classes = vocab_size)[0]
    # Store
    Ximages.append(image)
    XSeq.append(in_seq)
    y.append(out_seq)
  # Ximages, XSeq, y = array(Ximages), array(XSeq), array(y)
  return [Ximages, XSeq, y]

"""##**Fit Model**"""

from keras.utils import plot_model
from keras.models import Model
from keras.layers import Input
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers import LSTM
from keras.layers import RepeatVector
from keras.layers import TimeDistributed
from keras.layers import Embedding
from keras.layers.merge import concatenate
from keras.layers.pooling import GlobalMaxPooling2D

"""###**Define Model**"""

# Define the captioning model
def define_model(vocab_size, max_length):
  # Features extractor (encoder)
  inputs1 = Input(shape = (7, 7, 512))
  fe1 = GlobalMaxPooling2D()(inputs1)
  fe2 = Dense(128, activation = 'relu')(fe1)
  fe3 = RepeatVector(max_length)(fe2)  # Repeat input mx_length times
  # Embedding
  inputs2 = Input(shape = (max_length,))
  emb2 = Embedding(vocab_size, 50, mask_zero = True)(inputs2)
  emb3 = LSTM(256, return_sequences = True)(emb2)
  emb4 = TimeDistributed(Dense(128, activation = 'relu'))(emb3)
  # Merge Inputs
  merged = concatenate([fe3, emb4])
  # Language model (decoder)
  lm2 = LSTM(500)(merged)
  lm3 = Dense(500, activation = 'relu')(lm2)
  outputs = Dense(vocab_size, activation = 'softmax')(lm3)
  # Tie it together [image, seq] [word]
  model = Model(inputs = [inputs1, inputs2], outputs=outputs)
  model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])
  print(model.summary())
#  plot_model(model, show_shapes=True, to_file='plot.png')
  return model

"""###**Data Generator**"""

# Data generator, intended to be used in a call to model.fit_generator()
def data_generator(descriptions, features, tokenizer, max_length, n_step):
	# loop until we finish training
	while 1:
		# Loop over photo identifiers in the dataset
		keys = list(descriptions.keys())
		for i in range(0, len(keys), n_step):
			Ximages, XSeq, y = list(), list(),list()
			for j in range(i, min(len(keys), i+n_step)):
				image_id = keys[j]
				# Retrieve photo feature input
				image = features[image_id][0]
				# Retrieve text input
				desc = descriptions[image_id]
				# Generate input-output pairs
				in_img, in_seq, out_word = create_sequences(tokenizer, desc, image, max_length)
				for k in range(len(in_img)):
					Ximages.append(in_img[k])
					XSeq.append(in_seq[k])
					y.append(out_word[k])
			# Yield this batch of samples to the model
			yield [[array(Ximages), array(XSeq)], array(y)]

# Map an integer to a word
def word_for_id(integer, tokenizer):
	for word, index in tokenizer.word_index.items():
		if index == integer:
			return word
	return None

"""###**Generate Description**"""

# Generate a description for an image
def generate_desc(model, tokenizer, photo, max_length):
	# seed the generation process
	in_text = 'startseq'
	# Iterate over the whole length of the sequence
	for i in range(max_length):
		# Integer encode input sequence
		sequence = tokenizer.texts_to_sequences([in_text])[0]
		# Pad input
		sequence = pad_sequences([sequence], maxlen=max_length)
		# Predict next word
		yhat = model.predict([photo,sequence], verbose=0)
		# Convert probability to integer
		yhat = argmax(yhat)
		# Map integer to word
		word = word_for_id(yhat, tokenizer)
		# Stop if we cannot map the word
		if word is None:
			break
		# Append as input for generating the next word
		in_text += ' ' + word
		# Stop if we predict the end of the sequence
		if word == 'endseq':
			break
	return in_text

"""###**Evaluate Model Skill**"""

# Evaluate the skill of the model
def evaluate_model(model, descriptions, photos, tokenizer, max_length):
	actual, predicted = list(), list()
	# Step over the whole set
	for key, desc in descriptions.items():
		# Generate description
		yhat = generate_desc(model, tokenizer, photos[key], max_length)
		# Store actual and predicted
		actual.append([desc.split()])
		predicted.append(yhat.split())
	# Calculate BLEU score
	bleu = corpus_bleu(actual, predicted)
	return bleu

"""###**Fit Model**"""

# Define experiment
model_name = 'baseline1'
verbose = 1
n_epochs = 50
n_photos_per_update = 2
n_batches_per_epoch = int(len(train) / n_photos_per_update)
n_repeats = 3
max_length = 25 #max(len(s.split()) for s in list(train_descriptions.values()))

!apt-get -qq install python-pydot python-pydot-ng graphviz 
import pydot

for i in range(n_repeats):
	# Define the model
	model = define_model(vocab_size, max_length)
	# Fit model
	model.fit_generator(data_generator(train_descriptions, train_features, tokenizer, max_length, n_photos_per_update), steps_per_epoch=n_batches_per_epoch, epochs=n_epochs, verbose=verbose)
	# Evaluate model on training data
	train_score = evaluate_model(model, train_descriptions, train_features, tokenizer, max_length)
	test_score = evaluate_model(model, test_descriptions, test_features, tokenizer, max_length)
	# Store
	train_results.append(train_score)
	test_results.append(test_score)
	print('>%d: train=%f test=%f' % ((i+1), train_score, test_score))

# Save results to file
df = DataFrame()
df['train'] = train_results
df['test'] = test_results
print(df.describe())
df.to_csv(model_name+'.csv', index=False)

